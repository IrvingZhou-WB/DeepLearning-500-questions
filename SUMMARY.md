# Summary

- [前言](README.md)

- [第一章 数学基础](MarkDown/chapter01/preface.md)
  - [1.1 标量、向量、张量之间的联系](MarkDown/chapter01/1.1.md)
  - [1.2 张量与矩阵的区别？](MarkDown/chapter01/1.2.md)
  - [1.3 矩阵和向量相乘结果](MarkDown/chapter01/1.3.md)
  - [1.4 向量和矩阵的范数归纳](MarkDown/chapter01/1.4.md)
  - [1.5 如何判断一个矩阵为正定？](MarkDown/chapter01/1.5.md)
  - [1.6 导数偏导计算](MarkDown/chapter01/1.5.md)
  - [1.7 导数和偏导数有什么区别？](MarkDown/chapter01/1.6.md)
  - [1.8 特征值分解与特征向量](MarkDown/chapter01/1.7.md)
  - [1.9 奇异值与特征值有什么关系？](MarkDown/chapter01/1.8.md)
  - [1.10 机器学习为什么要使用概率？](MarkDown/chapter01/1.10.md)
  - [1.11 变量与随机变量有什么区别？](MarkDown/chapter01/1.11.md)
  - [1.12 常见概率分布？](MarkDown/chapter01/1.12.md)
  - [1.13 举例理解条件概率](MarkDown/chapter01/1.13.md)
  - [1.14 联合概率与边缘概率联系区别？](MarkDown/chapter01/1.14.md)
  - [1.15 条件概率的链式法则](MarkDown/chapter01/1.15.md)
  - [1.16 独立性和条件独立性](MarkDown/chapter01/1.16.md)
  - [1.17 期望、方差、协方差、相关系数总结](MarkDown/chapter01/1.17.md)

- [第二章 机器学习基础](MarkDown/chapter02/preface.md)
  - [2.1 各种常见算法图示](MarkDown/chapter02/2.1.md)
  - [2.2监督学习、非监督学习、半监督学习、弱监督学习？](MarkDown/chapter02/2.2.md)
  - [2.3 监督学习有哪些步骤](MarkDown/chapter02/2.3.md)
  - [2.4 多实例学习？](MarkDown/chapter02/2.4.md)
  - [2.5 分类网络和回归的区别？](MarkDown/chapter02/2.5.md)
  - [2.6 什么是神经网络？](MarkDown/chapter02/2.6.md)
  - [2.7 常用分类算法的优缺点？](MarkDown/chapter02/2.7.md)
  - [2.8 正确率能很好的评估分类算法吗？](MarkDown/chapter02/2.8.md)
  - [2.9 分类算法的评估方法？](MarkDown/chapter02/2.9.md)
  - [2.10 什么样的分类器是最好的？](MarkDown/chapter02/2.10.md)
  - [2.11大数据与深度学习的关系](MarkDown/chapter02/2.11.md)
  - [2.12 理解局部最优与全局最优](MarkDown/chapter02/2.12.md)
  - [2.13 理解逻辑回归](MarkDown/chapter02/2.13.md)
  - [2.14 逻辑回归与朴素贝叶斯有什么区别？](MarkDown/chapter02/2.14.md)
  - [2.16 代价函数作用原理](MarkDown/chapter02/2.16.md)
  - [2.17 为什么代价函数要非负？](MarkDown/chapter02/2.17.md)
  - [2.18 常见代价函数？](MarkDown/chapter02/2.18.md)
  - [2.19为什么用交叉熵代替二次代价函数](MarkDown/chapter02/2.19.md)
  - [2.20 什么是损失函数？](MarkDown/chapter02/2.20.md)
  - [2.21 常见的损失函数](MarkDown/chapter02/2.21.md)
  - [0.00 对数损失函数是如何度量损失的？](MarkDown/chapter02/2.00.md)
  - [2.23 机器学习中为什么需要梯度下降？](MarkDown/chapter02/2.23.md)
  - [2.24 梯度下降法缺点？](MarkDown/chapter02/2.24.md)
  - [2.25 梯度下降法直观理解？](MarkDown/chapter02/2.25.md)
  - [2.22 逻辑回归为什么使用对数损失函数？](MarkDown/chapter02/2.22.md)
  - [2.24 如何对梯度下降法进行调优？](MarkDown/chapter02/2.24.md)
  - [2.25 随机梯度和批量梯度区别？](MarkDown/chapter02/2.25.md)
  - [2.26 各种梯度下降法性能比较](MarkDown/chapter02/2.26.md)
  - [2.28 线性判别分析（LDA）思想总结](MarkDown/chapter02/2.28.md)
  - [2.29 图解LDA核心思想](MarkDown/chapter02/2.29.md)
  - [2.30 二类LDA算法原理？](MarkDown/chapter02/2.30.md)
  - [2.30 LDA算法流程总结？](MarkDown/chapter02/2.30.md)
  - [2.31 LDA和PCA区别？](MarkDown/chapter02/2.31.md)
  - [2.32 LDA优缺点？](MarkDown/chapter02/2.32.md)
  - [2.33 主成分分析（PCA）思想总结](MarkDown/chapter02/2.33.md)
  - [2.34 图解PCA核心思想](MarkDown/chapter02/2.34.md)
  - [2.35 PCA算法推理](MarkDown/chapter02/2.35.md)
  - [2.36 PCA算法流程总结](MarkDown/chapter02/2.36.md)
  - [2.37 PCA算法主要优缺点](MarkDown/chapter02/2.37.md)
  - [2.38 降维的必要性及目的](MarkDown/chapter02/2.38.md)
  - [2.39 KPCA与PCA的区别？](MarkDown/chapter02/2.39.md)
  - [2.40模型评估](MarkDown/chapter02/2.40.md)
    - [2.40.1模型评估常用方法？](MarkDown/chapter02/2.40.1.md)
    - [2.40.2 经验误差与泛化误差](MarkDown/chapter02/2.40.2.md)
    - [2.40.3 图解欠拟合、过拟合](MarkDown/chapter02/2.40.3.md)
    - [2.40.4 如何解决过拟合与欠拟合？](MarkDown/chapter02/2.40.1.md)
    - [2.40.5 交叉验证的主要作用？](MarkDown/chapter02/2.40.1.md)
    - [2.40.6 k折交叉验证？](MarkDown/chapter02/2.40.1.md)
    - [2.40.7 混淆矩阵](MarkDown/chapter02/2.40.1.md)
    - [2.40.1模型评估常用方法？](MarkDown/chapter02/2.40.1.md)
    - [2.40.8 错误率及精度](MarkDown/chapter02/2.40.1.md)
    - [2.40.9 查准率与查全率](MarkDown/chapter02/2.40.1.md)
    - [2.40.10 ROC与AUC](MarkDown/chapter02/2.40.1.md)
    - [2.40.11如何画ROC曲线？](MarkDown/chapter02/2.40.1.md)
    - [2.40.12如何计算TPR，FPR？](MarkDown/chapter02/2.40.1.md)
    - [2.40.13如何计算Auc？](MarkDown/chapter02/2.40.1.md)
    - [2.40.14为什么使用Roc和Auc评价分类器？](MarkDown/chapter02/2.40.1.md)
    - [2.40.15 直观理解AUC](MarkDown/chapter02/2.40.1.md)
    - [2.40.16 代价敏感错误率与代价曲线](MarkDown/chapter02/2.40.1.md)
    - [2.40.17 模型有哪些比较检验方法](MarkDown/chapter02/2.40.1.md)
    - [2.40.18 偏差与方差](MarkDown/chapter02/2.40.1.md)
    - [2.40.19为什么使用标准差？](MarkDown/chapter02/2.40.1.md)
    - [2.40.20 点估计思想](MarkDown/chapter02/2.40.1.md)
    - [2.40.21 点估计优良性原则？](MarkDown/chapter02/2.40.1.md)
    - [2.40.22点估计、区间估计、中心极限定理之间的联系？](MarkDown/chapter02/2.40.1.md)
    - [2.40.23 类别不平衡产生原因？](MarkDown/chapter02/2.40.1.md)
    - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.41 决策树](MarkDown/chapter02/2.40.1.md)
  - [2.41 支持向量机](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)
  - [2.40.24 常见的类别不平衡问题解决方法](MarkDown/chapter02/2.40.1.md)

- [第三章 深度学习基础](MarkDown/chapter03/preface.md)
  - [3.1 基本概念](MarkDown/chapter03/3.1.md)
    - [3.1.1 神经网络组成？](MarkDown/chapter03/3.1.md)
    - [3.1.2 神经网络有哪些常用模型结构？](MarkDown/chapter03/3.1.md)
    - [3.1.3 如何选择深度学习开发平台？](MarkDown/chapter03/3.1.md)
    - [3.1.4 为什么使用深层表示](MarkDown/chapter03/3.1.md)
    - [3.1.5 为什么深层神经网络难以训练？](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
  - [3.2 网络操作与计算](MarkDown/chapter03/3.2.md)
    - [3.2.1 前向传播与反向传播？](MarkDown/chapter03/3.1.md)
    - [3.2.2 如何计算神经网络的输出？](MarkDown/chapter03/3.1.md)
    - [3.2.3 如何计算卷积神经网络输出值？](MarkDown/chapter03/3.1.md)
    - [3.2.4 如何计算Pooling层输出值输出值？](MarkDown/chapter03/3.1.md)
    - [3.2.5 实例理解反向传播](MarkDown/chapter03/3.1.md)
  - [3.3 超参数](MarkDown/chapter03/3.3.md)
    - [3.3.1 什么是超参数？](MarkDown/chapter03/3.1.md)
    - [3.3.2 如何寻找超参数的最优值？](MarkDown/chapter03/3.1.md)
    - [3.3.3 超参数搜索一般过程？](MarkDown/chapter03/3.1.md)
  - [3.4 激活函数](MarkDown/chapter03/3.4.md)
    - [3.4.1 为什么需要非线性激活函数？](MarkDown/chapter03/3.1.md)
    - [3.4.2 常见的激活函数及图像](MarkDown/chapter03/3.1.md)
    - [3.4.3 常见激活函数的导数计算？](MarkDown/chapter03/3.1.md)
    - [3.4.4 激活函数有哪些性质？](MarkDown/chapter03/3.1.md)
    - [3.4.5 如何选择激活函数？](MarkDown/chapter03/3.1.md)
    - [3.4.6 使用ReLu激活函数的优点？](MarkDown/chapter03/3.1.md)
    - [3.4.7 什么时候可以用线性激活函数？](MarkDown/chapter03/3.1.md)
    - [3.4.8 怎样理解Relu（<0时）是非线性激活函数？](MarkDown/chapter03/3.1.md)
    - [3.4.9 Softmax函数如何应用于多分类？](MarkDown/chapter03/3.1.md)
  - [3.5 Batch_Size](MarkDown/chapter03/3.5.md)
    - [3.5.1 为什么需要Batch_Size？](MarkDown/chapter03/3.1.md)
    - [3.5.2 Batch_Size值的选择](MarkDown/chapter03/3.1.md)
    - [3.5.3 在合理范围内，增大 Batch_Size 有何好处？](MarkDown/chapter03/3.1.md)
    - [3.5.4 盲目增大 Batch_Size 有何坏处？](MarkDown/chapter03/3.1.md)
    - [3.5.5 调节 Batch_Size 对训练效果影响到底如何？](MarkDown/chapter03/3.1.md)
  - [3.6 归一化](MarkDown/chapter03/3.6.md)
    - [3.6.1 归一化含义？](MarkDown/chapter03/3.1.md)
    - [3.6.2 为什么要归一化](MarkDown/chapter03/3.1.md)
    - [3.6.3 为什么归一化能提高求解最优解速度？](MarkDown/chapter03/3.1.md)
    - [3.6.4 3D图解未归一化](MarkDown/chapter03/3.1.md)
    - [3.6.5 归一化有哪些类型？](MarkDown/chapter03/3.1.md)
    - [3.6.6 局部响应归一化作用](MarkDown/chapter03/3.1.md)
    - [3.6.7 理解局部响应归一化公式](MarkDown/chapter03/3.1.md)
    - [3.6.8 什么是批归一化（Batch Normalization）](MarkDown/chapter03/3.1.md)
    - [3.6.9 批归一化（BN）算法的优点](MarkDown/chapter03/3.1.md)
    - [3.6.10 批归一化（BN）算法流程](MarkDown/chapter03/3.1.md)
    - [3.6.11 批归一化和群组归一化](MarkDown/chapter03/3.1.md)
    - [3.6.12 Weight Normalization和Batch Normalization](MarkDown/chapter03/3.1.md)
  - [3.7 预训练与微调(fine tuning)](MarkDown/chapter03/3.7md)
    - [3.7.1 为什么无监督预训练可以帮助深度学习？](MarkDown/chapter03/3.1.md)
    - [3.7.2 什么是模型微调fine tuning](MarkDown/chapter03/3.1.md)
    - [3.7.3 微调时候网络参数是否更新？](MarkDown/chapter03/3.1.md)
    - [3.7.4 fine-tuning模型的三种状态](MarkDown/chapter03/3.1.md)
  - [3.8 权重偏差初始化](MarkDown/chapter03/3.8.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
    - [3.1.6 深度学习和机器学习有什么不同](MarkDown/chapter03/3.1.md)
  - [3.9 Softmax](MarkDown/chapter03/3.9.md)
    - [3.9.1 Softmax定义及作用](MarkDown/chapter03/3.1.md)
    - [3.9.2 Softmax推导](MarkDown/chapter03/3.1.md)
   - [3.10 理解One Hot Encodeing原理及作用？](MarkDown/chapter03/3.10.md)
   - [3.11 常用的优化器有哪些](MarkDown/chapter03/3.11.md)
   - [3.12 Dropout 系列问题](MarkDown/chapter03/3.12.md)
      - [3.12.1 dropout率的选择](MarkDown/chapter03/3.1.md)
   - [3.27 Padding 系列问题](MarkDown/chapter03/3.27.md)

- [第四章 经典网络](MarkDown/chapter04/preface.md)
  - [4.1 LetNet5](MarkDown/chapter01/1.1.md)
    - [4.1.1 模型结构](MarkDown/chapter01/1.1.md)
    - [4.1.1 模型解读](MarkDown/chapter01/1.1.md)
    - [4.1.3 模型特性](MarkDown/chapter01/1.1.md)
  - [4.2 AlexNet](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
  - [4.3 可视化ZFNet-解卷积](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
  - [4.4 VGG](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
  - [4.5 Network in Network](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
  - [4.6 GoogleNet](MarkDown/chapter01/1.1.md)
    - [4.1 AlexNet](MarkDown/chapter01/1.1.md)
  - [4.7 Inception 系列](MarkDown/chapter01/1.1.md)
    - [4.7.1 Inception v1](MarkDown/chapter01/1.1.md)
    - [4.7.2 Inception v2](MarkDown/chapter01/1.1.md)
    - [4.7.3 Inception v3](MarkDown/chapter01/1.1.md)
    - [4.7.4 Inception V4](MarkDown/chapter01/1.1.md)
    - [4.7.5 Inception-ResNet-v2](MarkDown/chapter01/1.1.md)
  - [4.8 ResNet及其变体](MarkDown/chapter01/1.1.md)
    - [4.8.1重新审视ResNet](MarkDown/chapter01/1.1.md)
    - [4.8.2 残差块](MarkDown/chapter01/1.1.md)
    - [4.8.3 ResNet架构](MarkDown/chapter01/1.1.md)
    - [4.8.4 残差块的变体](MarkDown/chapter01/1.1.md)
    - [4.8.5 ResNeXt](MarkDown/chapter01/1.1.md)
    - [4.8.6 Densely Connected CNN](MarkDown/chapter01/1.1.md)
    - [4.8.7 ResNet作为小型网络的组合](MarkDown/chapter01/1.1.md)
    - [4.8.8 ResNet中路径的特点](MarkDown/chapter01/1.1.md)
  - [4.9 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？](MarkDown/chapter01/1.1.md)

- [第五章 卷积神经网络(CNN)](MarkDown/chapter05/preface.md)
  - [5.1 卷积神经网络的组成层](MarkDown/chapter01/1.1.md)
  - [5.2 卷积如何检测边缘信息？](MarkDown/chapter01/1.1.md)
  - [5.2 卷积的几个基本定义？](MarkDown/chapter01/1.1.md)
  - [5.3 卷积网络类型分类？](MarkDown/chapter01/1.1.md)
  - [5.3 图解12种不同类型的2D卷积？](MarkDown/chapter01/1.1.md)
  - [5.4 2D卷积与3D卷积有什么区别？](MarkDown/chapter01/1.1.md)
  - [5.5 有哪些池化方法？](MarkDown/chapter01/1.1.md)
  - [5.6 1x1卷积作用？](MarkDown/chapter01/1.1.md)
  - [5.7 卷积层和池化层有什么区别？](MarkDown/chapter01/1.1.md)
  - [5.8 卷积核一定越大越好？](MarkDown/chapter01/1.1.md)
  - [5.9 每层卷积只能用一种尺寸的卷积核？](MarkDown/chapter01/1.1.md)
  - [5.10 怎样才能减少卷积层参数量？](MarkDown/chapter01/1.1.md)
  - [5.11 卷积操作时必须同时考虑通道和区域吗？](MarkDown/chapter01/1.1.md)
  - [5.12 采用宽卷积的好处有什么？](MarkDown/chapter01/1.1.md)
  - [5.13 卷积层输出的深度与哪个部件的个数相同？](MarkDown/chapter01/1.1.md)
  - [5.14 如何得到卷积层输出的深度？](MarkDown/chapter01/1.1.md)
  - [5.15 激活函数通常放在卷积神经网络的那个操作之后？](MarkDown/chapter01/1.1.md)
  - [5.16 如何理解最大池化层有几分缩小？](MarkDown/chapter01/1.1.md)
  - [5.17 理解图像卷积与反卷积](MarkDown/chapter01/1.1.md)
  - [5.18 不同卷积后图像大小计算？](MarkDown/chapter01/1.1.md)
  - [5.19 步长、填充大小与输入输出关系总结？](MarkDown/chapter01/1.1.md)
  - [5.20 理解反卷积和棋盘效应](MarkDown/chapter01/1.1.md)
  - [5.21 CNN主要的计算瓶颈？](MarkDown/chapter01/1.1.md)
  - [5.22 CNN的参数经验设置](MarkDown/chapter01/1.1.md)
  - [5.23 提高泛化能力的方法总结](MarkDown/chapter01/1.1.md)
  - [5.24 CNN在CV与NLP领域运用的联系与区别？](MarkDown/chapter01/1.1.md)
  - [5.25 CNN凸显共性的手段？](MarkDown/chapter01/1.1.md)
  - [5.26 全卷积与Local-Conv的异同点](MarkDown/chapter01/1.1.md)
  - [5.27 举例理解Local-Conv的作用](MarkDown/chapter01/1.1.md)
  - [5.28 简述卷积神经网络进化史](MarkDown/chapter01/1.1.md)

- [第六章 循环神经网络(RNN)](MarkDown/chapter06/preface.md)
  - [6.1 RNNs和FNNs有什么区别？](MarkDown/chapter01/1.1.md)
  - [6.2 RNNs典型特点？](MarkDown/chapter01/1.1.md)
  - [6.3 RNNs能干什么？](MarkDown/chapter01/1.1.md)
  - [6.4 RNNs在NLP中典型应用？](MarkDown/chapter01/1.1.md)
  - [6.5 RNNs训练和传统ANN训练异同点？](MarkDown/chapter01/1.1.md)
  - [6.6 常见的RNNs扩展和改进模型](MarkDown/chapter01/1.1.md)
    - [6.6.1 Simple RNNs(SRNs)](MarkDown/chapter01/1.1.md)
    - [6.6.2 Bidirectional RNNs](MarkDown/chapter01/1.1.md)
    - [6.6.3 Deep(Bidirectional) RNNs](MarkDown/chapter01/1.1.md)
    - [6.6.4 Echo State Networks（ESNs）](MarkDown/chapter01/1.1.md)
    - [6.6.5 Gated Recurrent Unit Recurrent Neural Networks](MarkDown/chapter01/1.1.md)
    - [6.6.6 LSTM Netwoorks](MarkDown/chapter01/1.1.md)
    - [6.6.7 Clockwork RNNs(CW-RNNs)](MarkDown/chapter01/1.1.md)

- [第七章 目标检测](MarkDown/chapter07/preface.md)
  - [7.1 基于候选区域的目标检测器](MarkDown/chapter01/1.1.md)
  - [7.2 基于区域的全卷积神经网络（R-FCN）](MarkDown/chapter01/1.1.md)
  - [7.3 单次目标检测器](MarkDown/chapter01/1.1.md)
  - [7.4 YOLO系列](MarkDown/chapter01/1.1.md)
    - [7.4.1 YOLOv1介绍](MarkDown/chapter01/1.1.md)
    - [7.4.2 YOLOv1模型优缺点？](MarkDown/chapter01/1.1.md)
    - [7.4.3 YOLOv2](MarkDown/chapter01/1.1.md)
    - [7.4.4 YOLOv2改进策略](MarkDown/chapter01/1.1.md)
    - [7.4.5 YOLOv2的训练](MarkDown/chapter01/1.1.md)
    - [7.4.6 YOLO9000](MarkDown/chapter01/1.1.md)
    - [7.4.7 YOLOv3](MarkDown/chapter01/1.1.md)
    - [7.4.8 YOLOv3改进](MarkDown/chapter01/1.1.md)

- [第八章 图像分割](MarkDown/chapter08/preface.md)
  - [8.1 传统的基于CNN的分割方法缺点？](MarkDown/chapter01/1.1.md)
  - [8.1 FCN](MarkDown/chapter01/1.1.md)
  - [8.2 U-Net](MarkDown/chapter01/1.1.md)
  - [8.3 SegNet](MarkDown/chapter01/1.1.md)
  - [8.4 空洞卷积(Dilated Convolutions)](MarkDown/chapter01/1.1.md)
  - [8.4 RefineNet](MarkDown/chapter01/1.1.md)
  - [8.5 PSPNet](MarkDown/chapter01/1.1.md)
  - [8.6 DeepLab系列](MarkDown/chapter01/1.1.md)
  - [8.7 Mask-R-CNN](MarkDown/chapter01/1.1.md)
  - [8.8 CNN在基于弱监督学习的图像分割中的应用](MarkDown/chapter01/1.1.md)

- [第九章 强化学习](MarkDown/chapter09/preface.md)
  - [9.1 强化学习的主要特点？](MarkDown/chapter01/1.1.md)
  - [9.2 强化学习应用实例](MarkDown/chapter01/1.1.md)
  - [9.3 强化学习和监督式学习、非监督式学习的区别](MarkDown/chapter01/1.1.md)
  - [9.4 强化学习主要有哪些算法？](MarkDown/chapter01/1.1.md)
  - [9.5 深度迁移强化学习算法](MarkDown/chapter01/1.1.md)
  - [9.6 分层深度强化学习算法](MarkDown/chapter01/1.1.md)
  - [9.7 深度记忆强化学习算法](MarkDown/chapter01/1.1.md)
  - [9.8 多智能体深度强化学习算法](MarkDown/chapter01/1.1.md)
  - [9.9 深度强化学习算法小结](MarkDown/chapter01/1.1.md)

- [第十章 迁移学习](MarkDown/chapter10/preface.md)
  - [10.1 什么是迁移学习？](MarkDown/chapter01/1.1.md)
  - [10.2 什么是多任务学习？](MarkDown/chapter01/1.1.md)
  - [10.3 多任务学习有什么意义？](MarkDown/chapter01/1.1.md)
  - [10.4 什么是端到端的深度学习？](MarkDown/chapter01/1.1.md)
  - [10.5 端到端的深度学习举例？](MarkDown/chapter01/1.1.md)
  - [10.6 端到端的深度学习有什么挑战？](MarkDown/chapter01/1.1.md)
  - [10.7 端到端的深度学习优缺点？](MarkDown/chapter01/1.1.md)

- [第十三章 优化算法](MarkDown/chapter13/preface.md)
  - [13.1 CPU和GPU 的区别？](MarkDown/chapter01/1.1.md)
  - [13.2 如何解决训练样本少的问题](MarkDown/chapter01/1.1.md)
  - [13.3 什么样的样本集不适合用深度学习?](MarkDown/chapter01/1.1.md)
  - [13.4 有没有可能找到比已知算法更好的算法?](MarkDown/chapter01/1.1.md)
  - [13.5 何为共线性, 跟过拟合有啥关联?](MarkDown/chapter01/1.1.md)
  - [13.6 广义线性模型是怎被应用在深度学习中?](MarkDown/chapter01/1.1.md)
  - [13.7 造成梯度消失的原因?](MarkDown/chapter01/1.1.md)
  - [13.8 权值初始化方法有哪些](MarkDown/chapter01/1.1.md)
  - [13.9 启发式优化算法中，如何避免陷入局部最优解？](MarkDown/chapter01/1.1.md)
  - [13.10 凸优化中如何改进GD方法以防止陷入局部最优解](MarkDown/chapter01/1.1.md)
  - [13.11 常见的损失函数？](MarkDown/chapter01/1.1.md)
  - [13.14 如何进行特征选择（feature selection）？](MarkDown/chapter01/1.1.md)
  - [13.15 梯度消失/梯度爆炸原因，以及解决方法](MarkDown/chapter01/1.1.md)
  - [13.16 深度学习为什么不用二阶优化](MarkDown/chapter01/1.1.md)
  - [13.17 怎样优化你的深度学习系统？](MarkDown/chapter01/1.1.md)
  - [13.18 为什么要设置单一数字评估指标？](MarkDown/chapter01/1.1.md)
  - [13.19 满足和优化指标（Satisficing and optimizing metrics）](MarkDown/chapter01/1.1.md)
  - [13.20 怎样划分训练/开发/测试集](MarkDown/chapter01/1.1.md)
  - [13.21 如何划分开发/测试集大小](MarkDown/chapter01/1.1.md)
  - [13.22 什么时候该改变开发/测试集和指标？](MarkDown/chapter01/1.1.md)
  - [13.23 设置评估指标的意义？](MarkDown/chapter01/1.1.md)
  - [13.24 什么是可避免偏差？](MarkDown/chapter01/1.1.md)
  - [13.25 什么是TOP5错误率？](MarkDown/chapter01/1.1.md)
  - [13.26 什么是人类水平错误率？](MarkDown/chapter01/1.1.md)
  - [13.27 可避免偏差、几大错误率之间的关系？](MarkDown/chapter01/1.1.md)
  - [13.28 怎样选取可避免偏差及贝叶斯错误率？](MarkDown/chapter01/1.1.md)
  - [13.29 怎样减少方差？](MarkDown/chapter01/1.1.md)
  - [13.30 贝叶斯错误率的最佳估计](MarkDown/chapter01/1.1.md)
  - [13.31 举机器学习超过单个人类表现几个例子？](MarkDown/chapter01/1.1.md)
  - [13.32 如何改善你的模型？](MarkDown/chapter01/1.1.md)
  - [13.33 理解误差分析](MarkDown/chapter01/1.1.md)
  - [13.34 为什么值得花时间查看错误标记数据？](MarkDown/chapter01/1.1.md)
  - [13.36 为什么要在不同的划分上训练及测试？](MarkDown/chapter01/1.1.md)
  - [13.37 如何解决数据不匹配问题？](MarkDown/chapter01/1.1.md)
  - [13.38 梯度检验注意事项？](MarkDown/chapter01/1.1.md)
  - [13.39 什么是随机梯度下降？](MarkDown/chapter01/1.1.md)
  - [13.40 什么是批量梯度下降？](MarkDown/chapter01/1.1.md)
  - [13.41 什么是小批量梯度下降？](MarkDown/chapter01/1.1.md)
  - [13.42 怎么配置mini-batch梯度下降](MarkDown/chapter01/1.1.md)
  - [13.43 局部最优的问题](MarkDown/chapter01/1.1.md)
  - [13.37 如何解决数据不匹配问题？](MarkDown/chapter01/1.1.md)
  - [13.44 提升算法性能思路](MarkDown/chapter01/1.1.md)

- [第十四章 超参数调整](MarkDown/chapter14/preface.md)
  - [14.1 调试处理](MarkDown/chapter01/1.1.md)
  - [14.2 有哪些超参数](MarkDown/chapter01/1.1.md)
  - [14.3 如何选择调试值?](MarkDown/chapter01/1.1.md)
  - [14.4 为超参数选择合适的范围](MarkDown/chapter01/1.1.md)
  - [14.5 如何搜索超参数？](MarkDown/chapter01/1.1.md)

- [第十五章 正则化](MarkDown/chapter15/preface.md)
  - [15.1 什么是正则化？](MarkDown/chapter01/1.1.md)
  - [15.2 正则化原理？](MarkDown/chapter01/1.1.md)
  - [15.3 为什么要正则化？](MarkDown/chapter01/1.1.md)
  - [15.4 为什么正则化有利于预防过拟合？](MarkDown/chapter01/1.1.md)
  - [15.5 为什么正则化可以减少方差？](MarkDown/chapter01/1.1.md)
  - [15.6 L2正则化的理解？](MarkDown/chapter01/1.1.md)
  - [15.7 理解dropout 正则化](MarkDown/chapter01/1.1.md)
  - [15.8 有哪些dropout 正则化方法？](MarkDown/chapter01/1.1.md)
  - [15.8 如何实施dropout 正则化](MarkDown/chapter01/1.1.md)
  - [15.9 Python 实现dropout 正则化](MarkDown/chapter01/1.1.md)
  - [15.10 L2正则化和dropout 有什么不同？](MarkDown/chapter01/1.1.md)
  - [15.11 dropout有什么缺点？](MarkDown/chapter01/1.1.md)
  - [15.12 其他正则化方法？](MarkDown/chapter01/1.1.md)

- [参考文献](MarkDown/第一章_数学基础.md)